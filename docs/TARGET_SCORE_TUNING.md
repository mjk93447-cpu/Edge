# 목표 점수별 학습 전략 튜닝

## 1. 개요

사용자가 원하는 점수(0.3/0.4/0.5/0.6 등)를 입력하면, 해당 점수를 **가능한 한 빠르게** 달성하도록 학습 전략을 자동 튜닝한다.

- **영점삼(0.3)**: 빠른 조기 종료, 소량 예산.
- **영점사(0.4)**: 중간 예산·라운드, 적당한 조기 종료.
- **영점오(0.5)**: 더 많은 예산·라운드, phase 비율 조정.
- **영점육(0.6)**: 가장 보수적(많은 평가, 더 많은 라운드 유지).

## 2. 사용자 목표 점수 입력

### GUI

- **Auto Optimize** 클릭 후, 이미지 타입(그레이스케일/저화질) 선택 다음에 **목표 점수** 대화상자가 뜬다.
- **0.3 (영점삼) / 0.4 (영점사) / 0.5 (영점오) / 0.6 (영점육)** 중 선택.
- 선택한 값에 따라 `get_strategy_for_target_score(target)` 로 전략이 정해지고, 해당 전략이 auto_config에 반영된 뒤 최적화가 진행된다.

### CLI (단일 목표)

```bash
python run_target_score_tuning.py
```

- 실행 후 프롬프트에서 목표 점수 입력 (예: 0.4 또는 Enter로 기본 0.4).
- 해당 목표에 맞는 전략으로 1회 학습·평가 후 결과 출력.

### CLI (0.4 / 0.5 / 0.6 배치 + 3회 개선 루프)

```bash
python run_target_score_tuning.py --batch
```

- 목표 **0.4, 0.5, 0.6** 각각에 대해 **서로 다른 전략**으로 학습.
- **최소 3회** 반복:  
  **테스트 → 평가·현황 파악 → 개선 전략 도출 → 설정 반영 → 테스트**.

빠른 검증(예산 축소):

```bash
python run_target_score_tuning.py --batch --quick
```

## 3. 전략 튜닝 알고리즘

### 3.1 목표별 기본 전략 (`get_strategy_for_target_score`)

| 목표   | 전략명         | 조기종료(라운드) | no_improve_rounds | rounds_after | phase1_frac | phase1_min_evals | CLI 예산 | 라운드 크기 |
|--------|----------------|------------------|-------------------|--------------|-------------|------------------|----------|-------------|
| ≤0.35  | aggressive_low | 0.20             | 1                 | 0            | 0.4         | 80               | 250      | 30          |
| ≤0.45  | medium_04      | 0.22             | 2                 | 1            | 0.45        | 120              | 400      | 35          |
| ≤0.55  | medium_05      | 0.25             | 2                 | 1            | 0.5         | 150              | 600      | 40          |
| >0.55  | careful_06      | 0.28             | 3                 | 2            | 0.55        | 200              | 900      | 45          |

- **rounds_after**: 목표 점수 도달 후 추가로 돌릴 라운드 수 (도달 직후 곧바로 멈출지, 한두 라운드 더 돌릴지).
- **phase1_frac / phase1_min_evals**: 2단계 학습(Phase1 비-thinning → Phase2 thinning)에서 Phase1 비중.

### 3.2 개선 루프 (배치 모드)

매 반복에서:

1. **테스트**: 0.4, 0.5, 0.6 각각 현재 전략 오버라이드(`target_score_strategy_config.json`)로 학습 실행.
2. **평가·현황 파악**: 목표별로 도달 여부, 소요 시간, 평가 횟수 집계.
3. **개선 전략 도출**:
   - **도달 실패**: `eval_budget` ×1.4 (상한 1200), `round_size` +5 (상한 50).
   - **도달했으나 시간 과다**(평균 time_to_target > 120초): 예산·라운드 소폭 감소(×0.9, -2).
4. **설정 반영**: 목표별 `eval_budget`, `round_size`를 `target_score_strategy_config.json`에 저장하여 다음 루프에 사용.
5. 다음 루프에서 1번부터 반복.

결과는 `target_tuning_results_YYYYMMDD_HHMMSS.json`에 저장된다.

## 4. 결과물

- **GUI**: 사용자 선택 목표에 맞는 전략으로 Auto 최적화 실행, 목표 도달 시 설정에 따라 조기 종료.
- **CLI 단일**: 입력한 목표 1개에 대해 1회 학습·결과 출력.
- **CLI 배치**: 0.4/0.5/0.6 각각 다른 전략 + 3회 이상 테스트→평가→개선 루프, 최종 전략과 루프별 결과를 JSON으로 저장.

실제 데이터(FCB/실사진)에서는 동일 설정으로 0.3~0.6 구간을 더 빠르게 도달할 수 있도록 설계되어 있다. 테스트 이미지가 단순한 합성일 경우 점수가 낮게 나올 수 있으며, 이때도 개선 루프는 예산·라운드를 자동으로 늘려 재시도한다.

## 5. 실행 결과 예시 (3회 개선 루프)

- **1차**: 목표 0.4/0.5/0.6 각각 60 evals → 미도달 시 전략에서 eval_budget·round_size 증가.
- **2차**: 갱신된 예산·라운드로 재실행 → 다시 평가 후 필요 시 추가 증가.
- **3차**: 최종 예산·라운드로 실행 → `target_tuning_results_*.json` 및 `target_score_strategy_config.json`에 최종 전략 저장.

배치 실행 예:

```text
python run_target_score_tuning.py --batch
```

풀 예산으로 3회 루프를 돌리면, 목표별 도달 횟수와 평균 time_to_target이 JSON과 콘솔 요약에 출력된다.
